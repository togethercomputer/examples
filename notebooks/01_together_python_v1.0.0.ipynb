{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8e05d1-094c-4f17-905a-93edb5043cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "using TOGETHER_API_KEY ending in c04c\n",
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "import together\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')\n",
    "print(f\"using TOGETHER_API_KEY ending in {TOGETHER_API_KEY[-4:]}\")\n",
    "\n",
    "together.api_key = TOGETHER_API_KEY \n",
    "\n",
    "print(together.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f2e20e-7fb5-499e-a29b-96d33932a5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello! It's nice to meet you\n"
     ]
    }
   ],
   "source": [
    "client = together.Together(\n",
    "  api_key=TOGETHER_API_KEY\n",
    ")\n",
    "\n",
    "# Chat Completions\n",
    "response = client.chat.completions.create(\n",
    "    model=\"togethercomputer/llama-2-7b-chat\", \n",
    "    max_tokens=10, \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"hello there\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdeb837-7689-44c1-bbd4-1e7a8ba7dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I'm a newbie to the forum\n"
     ]
    }
   ],
   "source": [
    "# Completions\n",
    "response = client.completions.create(\n",
    "    model=\"togethercomputer/llama-2-7b\", \n",
    "    max_tokens=10, \n",
    "    prompt=\"hello there\"\n",
    ")\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a734340-d50c-44b5-a9ee-a3cdcb2506a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10394287, 0.16479492, -0.24951172, -0.025772095, -0.13427734, -0.026519775, 0.3930664, -0.2052002, -0.043304443, -0.10217285, 0.091796875, -0.041046143, -0.016296387, 0.1496582, -0.37646484, -0.21679688, -0.1842041, 0.23657227]\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "response = client.embeddings.create(model=\"bert-base-uncased\", input=[\"dog\", \"puppy\"])\n",
    "print(response.data[0].embedding[:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceac5986-d9cb-4df3-8b79-eb6b30894ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python_model_ready.ipynb',\n",
       " 'quick_start_v1.ipynb',\n",
       " 'check_new_models.ipynb',\n",
       " 'antihallucination.jsonl',\n",
       " '01_together_python_v1.0.0.ipynb',\n",
       " 'finetune_inputs.ipynb',\n",
       " 'together-finetune.ipynb',\n",
       " 'fastchat.ipynb',\n",
       " 'antihallucination_dataset.jsonl',\n",
       " '500_error_handling.ipynb',\n",
       " 'async-client.ipynb',\n",
       " 'together-poe-changes.ipynb',\n",
       " 'poker_analysis.jsonl',\n",
       " 'chat_api_v1.ipynb',\n",
       " 'files.ipynb',\n",
       " 'correctsample.jsonl',\n",
       " 'together-start-stop.ipynb',\n",
       " 'llama2-5xx-v2.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'inference_startup.ipynb']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d36468fc-4a78-42ca-9796-36f0fdd4bd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"file-4bbd2608-053e-4078-bc23-5af97df99134\",\n",
      "    \"object\": \"file\",\n",
      "    \"created_at\": 1710361599,\n",
      "    \"purpose\": \"fine-tune\",\n",
      "    \"filename\": \"antihallucination_dataset.jsonl\",\n",
      "    \"bytes\": 0,\n",
      "    \"line_count\": 0,\n",
      "    \"processed\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Upload Training File\n",
    "response = client.files.upload(\"antihallucination_dataset.jsonl\")\n",
    "print(json.dumps(response.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3456a138-3c9c-4f1d-98d2-abfbf7d5e7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"ft-81309c6d-6d12-46e1-897c-53a16c0c9f8c\",\n",
      "    \"training_file\": \"file-4bbd2608-053e-4078-bc23-5af97df99134\",\n",
      "    \"validation_file\": \"\",\n",
      "    \"model\": \"togethercomputer/llama-2-7b\",\n",
      "    \"output_name\": \"carson/llama-2-7b-2024-03-13-20-39-39\",\n",
      "    \"n_epochs\": 1,\n",
      "    \"n_checkpoints\": 1,\n",
      "    \"batch_size\": 32,\n",
      "    \"learning_rate\": 1e-05,\n",
      "    \"eval_steps\": 0,\n",
      "    \"lora\": false,\n",
      "    \"lora_r\": 8,\n",
      "    \"lora_alpha\": 8,\n",
      "    \"lora_dropout\": 0,\n",
      "    \"created_at\": \"2024-03-13T20:39:39.836Z\",\n",
      "    \"updated_at\": \"2024-03-13T20:39:39.836Z\",\n",
      "    \"status\": \"pending\",\n",
      "    \"job_id\": \"\",\n",
      "    \"events\": [\n",
      "        {\n",
      "            \"object\": \"fine-tune-event\",\n",
      "            \"created_at\": \"2024-03-13T20:39:39.836Z\",\n",
      "            \"level\": \"\",\n",
      "            \"message\": \"Fine tune request created\",\n",
      "            \"type\": \"JOB_PENDING\",\n",
      "            \"param_count\": 0,\n",
      "            \"token_count\": 0,\n",
      "            \"wandb_url\": \"\",\n",
      "            \"hash\": \"\",\n",
      "            \"checkpoint_path\": \"\",\n",
      "            \"model_path\": \"\",\n",
      "            \"training_offset\": 0\n",
      "        }\n",
      "    ],\n",
      "    \"token_count\": 0,\n",
      "    \"param_count\": 0,\n",
      "    \"total_price\": 0,\n",
      "    \"epochs_completed\": 0,\n",
      "    \"queue_depth\": 0,\n",
      "    \"wandb_project_name\": \"\",\n",
      "    \"wandb_url\": \"\",\n",
      "    \"TrainingFileNumLines\": 0,\n",
      "    \"TrainingFileSize\": 779612,\n",
      "    \"model_output_path\": \"s3://together-dev/finetune/64c4302a5cb247a0c80a3ddb/carson/llama-2-7b-2024-03-13-20-39-39/ft-81309c6d-6d12-46e1-897c-53a16c0c9f8c\",\n",
      "    \"Suffix\": \"\",\n",
      "    \"user_id\": \"64c4302a5cb247a0c80a3ddb\",\n",
      "    \"staring_epoch\": 0,\n",
      "    \"training_offset\": 0,\n",
      "    \"checkspoint_path\": \"\",\n",
      "    \"random_seed\": \"\",\n",
      "    \"owner_address\": \"0xef5286fc0a1ac5bc4d4221cf3d51f1d97c45eaf7\",\n",
      "    \"wandb_key\": \"\",\n",
      "    \"enable_checkpoints\": false,\n",
      "    \"internal_flags\": \"\",\n",
      "    \"UsedModelName\": \"\",\n",
      "    \"job_stats\": {\n",
      "        \"FtUserTime\": \"\",\n",
      "        \"FtSysTime\": \"\",\n",
      "        \"FtMaxRss\": 0,\n",
      "        \"FtMinPgFlt\": 0,\n",
      "        \"FtMajPgFlt\": 0,\n",
      "        \"FtInBlock\": 0,\n",
      "        \"FtOutBlock\": 0,\n",
      "        \"FtNvCsw\": 0,\n",
      "        \"FtNivCsw\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Fine Tuning\n",
    "response = client.fine_tuning.create(\n",
    "    training_file=\"file-4bbd2608-053e-4078-bc23-5af97df99134\", \n",
    "    model=\"togethercomputer/llama-2-7b\"\n",
    ")\n",
    "\n",
    "finetune_id = response.id\n",
    "print(json.dumps(response.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e57d470-1cad-4552-9a86-cdc5efcd1227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinetuneJobStatus.STATUS_PENDING\n"
     ]
    }
   ],
   "source": [
    "finetune_response = client.fine_tuning.retrieve(id=finetune_id)\n",
    "\n",
    "print(finetune_response.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6022ce73-9847-4670-b7a3-1b40e5c2dc10",
   "metadata": {},
   "source": [
    "# Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a925861b-9340-4a81-b78d-06ff49f616eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "123479a4-58e1-4761-a9f8-a0b54b9f39ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello! It's nice to meet you\n"
     ]
    }
   ],
   "source": [
    "```python\n",
    "\n",
    "async_client = together.AsyncTogether(\n",
    "  api_key=TOGETHER_API_KEY\n",
    ")\n",
    "\n",
    "# Chat Completions\n",
    "response = await async_client.chat.completions.create(\n",
    "    model=\"togethercomputer/llama-2-7b-chat\", \n",
    "    max_tokens=10, \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"hello there\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "```\n",
    "\n",
    "modify your answer above with the information above that this client is already asynchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e575fdd-5fd4-4a8c-bc97-f68dd55c8d4f",
   "metadata": {},
   "source": [
    "# Comaprison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01f885eb-f55c-4162-a1d4-d9e82c74606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from together import Together\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')\n",
    "\n",
    "def sync_chat_completion(messages, max_tokens):\n",
    "    client = Together(api_key=TOGETHER_API_KEY)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for message in messages:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"togethercomputer/llama-2-7b-chat\", \n",
    "            max_tokens=max_tokens, \n",
    "            messages=[{\"role\": \"user\", \"content\": message}]\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Synchronous total execution time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c47c6473-e229-42ff-ad45-35abc0a3f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from together import AsyncTogether\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')\n",
    "\n",
    "async def async_chat_completion(messages, max_tokens):\n",
    "    async_client = AsyncTogether(api_key=TOGETHER_API_KEY)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    tasks = [async_client.chat.completions.create(\n",
    "                model=\"togethercomputer/llama-2-7b-chat\", \n",
    "                max_tokens=max_tokens, \n",
    "                messages=[{\"role\": \"user\", \"content\": message}]\n",
    "             ) for message in messages]\n",
    "             \n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for response in responses:\n",
    "        print(response.choices[0].message.content)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Asynchronous total execution time:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "def run_async_example(messages):\n",
    "    asyncio.run(async_chat_completion(messages, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2399b7ab-54d3-4a87-9ebe-b0298f445958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The meaning of life is a question that has puzzled philosophers, theologians, and scientists for centuries. There are many different perspectives\n",
      "  Paris is located in France. It is the capital and largest city of France, situated in the northern central part of the country.\n",
      "Synchronous total execution time: 0.7738921642303467 seconds\n",
      "  The meaning of life is a question that has puzzled philosophers, theologians, and scientists for centuries. There are many different perspectives\n",
      "  Paris is located in France. It is the capital and largest city of France, situated in the northern central part of the country.\n",
      "Asynchronous total execution time: 0.4429478645324707 seconds\n"
     ]
    }
   ],
   "source": [
    "messages = [\"hi there what is the meaning of life?\", \"What country is Paris in?\"]\n",
    "sync_chat_completion(messages, 32)\n",
    "await async_chat_completion(messages, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a2e81-f1a2-4935-96ee-a1e40fa8d574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
